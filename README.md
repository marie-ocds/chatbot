# Doctor Dolittle RAG Chatbot

An intelligent question-answering system for "The Story of Doctor Dolittle" by Hugh Lofting, built using Retrieval-Augmented Generation (RAG) with LlamaIndex and ChromaDB.

## Overview

This chatbot implements a multi-level retrieval system that intelligently routes user queries to the most appropriate knowledge base:
- **Book-level**: For questions about overall themes, summary, and high-level analysis
- **Chapter-level**: For questions about specific chapters or chapter summaries
- **Scene-level**: For detailed questions about specific events, actions, or facts

## Features

- **Intelligent Query Routing**: Automatically determines the optimal retrieval strategy based on query type
- **Multi-Granularity Retrieval**: Three-tier indexing system (book/chapter/scene) for efficient and accurate responses
- **Optimized Performance**: Minimal similarity search overhead for single-document queries
- **Persistent Storage**: Uses ChromaDB for vector storage with automatic index persistence
- **Configurable Chunking**: Adjustable chunk sizes for scene-level retrieval

## Architecture

### Indexing System

1. **Book Index**: Contains a global summary generated by concatenating chapter summaries
2. **Chapter Index**: Stores individual chapter documents with metadata (chapter number, title)
3. **Scenes Index**: Fine-grained chunks of text for detailed factual queries

### Query Routing

The system uses an LLM-based router that:
- Analyzes the user query
- Determines the appropriate index (book/chapter/scene)
- Extracts chapter numbers if mentioned
- Routes the query to the optimal retrieval engine

### Retrieval Optimization

- Book and specific chapter queries use `similarity_top_k=1` (only one document exists)
- Scene queries use configurable `SCENE_SIMILARITY_TOP_K` for multi-document retrieval
- Metadata filtering for chapter-specific queries

## Installation

### Prerequisites

- Python 3.12+
- OpenAI API key

### Setup

1. Clone the repository:
```bash
git clone <repository-url>
cd project
```

2. Create a virtual environment:
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

3. Install dependencies:
```bash
pip install -r requirements.txt
```

4. Create a `.env` file in the project root:
```env
OPENAI_API_KEY=your_api_key_here
```

## Configuration

All configuration parameters are centralized in `src/config.py`:

```python
# LLM Configuration
LLM_MODEL = "gpt-4o-mini-2024-07-18"

# Embedding Configuration
EMBEDDING_MODEL = "text-embedding-3-small"

# Text splitting configuration
SCENE_CHUNK_SIZE = 600
SCENE_CHUNK_OVERLAP = 80

# Retrieval configuration
SCENE_SIMILARITY_TOP_K = 3
```

## Usage

### Running the Chatbot

```bash
python main.py
```

The chatbot will:
1. Load or build the vector indices (first run will take longer)
2. Start an interactive Q&A session
3. Accept natural language questions about the book

Example queries:
- "What is the main theme of the book?"
- "What happens in chapter 5?"
- "Who is Polynesia?"

Type `exit` or `quit` to end the session.

### Rebuilding the Scenes Index

If you modify `SCENE_CHUNK_SIZE` or `SCENE_CHUNK_OVERLAP`, rebuild only the scenes index:

```bash
python rebuild_scenes_index.py
```

This preserves the book and chapter indices while regenerating scene-level chunks.

## Project Structure

```
project/
├── book/
│   └── the-story-of-doctor-dolittle.pdf
├── src/
│   ├── config.py           # Configuration parameters
│   ├── data_loader.py      # PDF processing and document creation
│   ├── indexing.py         # Vector index management
│   ├── retrieval.py        # Query routing and retrieval
│   └── utils.py            # Text processing utilities
├── chroma_db/              # ChromaDB persistent storage (generated)
├── main.py                 # Main chatbot script
├── rebuild_scenes_index.py # Utility to rebuild scenes index
├── requirements.txt        # Python dependencies
├── .env                    # Environment variables (create this)
└── README.md              # This file
```

## Technical Details

### Dependencies

- **llama-index-core**: Core RAG framework
- **llama-index-embeddings-openai**: OpenAI embeddings integration
- **llama-index-llms-openai**: OpenAI LLM integration
- **llama-index-vector-stores-chroma**: ChromaDB vector store
- **chromadb**: Vector database
- **openai**: OpenAI API client
- **PyMuPDF**: PDF text extraction
- **pydantic**: Data validation
- **python-dotenv**: Environment variable management

### Text Processing Pipeline

1. **PDF Extraction**: Extract raw text using PyMuPDF
2. **Text Cleaning**: Remove formatting artifacts, normalize whitespace
3. **Chapter Detection**: Identify chapter boundaries and titles
4. **Document Creation**: Generate structured documents with metadata
5. **Chunking**: Split chapters into scenes using sentence-aware splitting
6. **Embedding**: Generate vector embeddings using OpenAI
7. **Indexing**: Store in ChromaDB for efficient retrieval

## Performance Optimizations

- **Single-document optimization**: Book and chapter queries skip unnecessary similarity calculations
- **Metadata filtering**: Efficient chapter-specific retrieval
- **Persistent storage**: Indices are loaded from disk after initial creation
- **Configurable chunking**: Balance between granularity and context

## Future Enhancements

- Add support for multiple books
- Implement conversation memory for follow-up questions
- Add citation extraction with page numbers
- Support for multimodal content (images from the book)
- Web interface using Streamlit or Gradio

## License

[Add license information]

## Acknowledgments

- Book: "The Story of Doctor Dolittle" by Hugh Lofting
- Framework: LlamaIndex
- Vector Database: ChromaDB
- LLM Provider: OpenAI